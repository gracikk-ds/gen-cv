

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 5e-4
  weight_decay: 1e-1

scheduler:
  _target_: src.utils.scheduler.WarmupMultiStepLR
  _partial_: true
  warmup_steps: 15
  initial_lr: 5e-6
  milestones: [50, 75]
  gamma: 0.5

schopt_handler:
  _target_: src.utils.scheduler.OptimizerSchedulerHandler
  optimizer: ${optimizer.optimizer}
  scheduler: ${optimizer.scheduler}

epochs: 100
max_grad_norm: 1.0
